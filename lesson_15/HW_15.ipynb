{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 2 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "\n",
      "WARNING: You are on Windows. If you detect any issue with pandarallel, be sure you checked out the Troubleshooting page:\n",
      "https://nalepae.github.io/pandarallel/troubleshooting/\n"
     ]
    }
   ],
   "source": [
    "from telegram.ext import Updater, CommandHandler, MessageHandler, Filters, CallbackContext\n",
    "import dialogflow\n",
    "from telegram import Update\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from stop_words import get_stop_words\n",
    "import string\n",
    "import annoy\n",
    "from gensim.models import Word2Vec, FastText\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразование файла аопросов-ответов в строчный вид\n",
    "if not os.path.isfile('prepared_answers.txt'):\n",
    "    \n",
    "    question = None\n",
    "    written = False\n",
    "    \n",
    "    with open(\"prepared_answers.txt\", \"w\", encoding=\"utf8\") as fout:  \n",
    "        with open(\"Otvety.txt\", \"r\", encoding=\"utf8\") as fin:\n",
    "            for line in tqdm(fin):\n",
    "                if line.startswith(\"---\"):\n",
    "                    written = False\n",
    "                    continue\n",
    "                if not written and question is not None:\n",
    "                    fout.write(question.replace(\"\\t\", \" \").strip() + \"===\" + line.replace(\"\\t\", \" \"))\n",
    "                    written = True\n",
    "                    question = None\n",
    "                    continue\n",
    "                if not written:\n",
    "                    question = line.strip()\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Препроцессинг текста\n",
    "def preprocess_txt(line):\n",
    "    spls = \"\".join(i for i in line.strip() if i not in exclude).split()\n",
    "    spls = [morpher.parse(i.lower())[0].normal_form for i in spls]\n",
    "    spls = [i for i in spls if i not in sw and i != \"\"]\n",
    "    return spls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для очистки текста из статьи https://habr.com/ru/articles/738176/ адаптированная под русский язык\n",
    "def clean_text(input_text):\n",
    "\n",
    "    # HTML-теги: первый шаг - удалить из входного текста все HTML-теги\n",
    "    clean_text = re.sub('<[^<]+?>', '', input_text)\n",
    "\n",
    "    # URL и ссылки: далее - удаляем из текста все URL и ссылки\n",
    "    clean_text = re.sub(r'http\\S+', '', clean_text)\n",
    "\n",
    "    #Эмоджи и эмотиконы: используем собственную функцию для преобразования эмоджи в текст\n",
    "    #Важно понимать эмоциональную окраску обрабатываемого текста\n",
    "    clean_text = emojis_words(clean_text)\n",
    "\n",
    "    # Приводим все входные данные к нижнему регистру\n",
    "    clean_text = clean_text.lower()\n",
    "\n",
    "    # Убираем все пробелы\n",
    "    # Так как все данные теперь представлены словами - удалим пробелы\n",
    "    clean_text = re.sub('\\s+', ' ', clean_text)\n",
    "\n",
    "    #Убираем специальные символы: избавляемся от всего, что не является \"словами\"\n",
    "    clean_text = re.sub('[^а-яА-ЯёЁ0-9\\s]', '', clean_text) #\n",
    "\n",
    "    # Записываем числа прописью: 100 превращается в \"сто\" (для компьютера)# не работает\n",
    "    temp = inflect.engine()\n",
    "    words = []\n",
    "    for word in clean_text.split():\n",
    "        if word.isdigit():\n",
    "            words.append(num2words(int(word), lang='ru'))\n",
    "        else:\n",
    "            words.append(word)\n",
    "    clean_text = ' '.join(words)\n",
    "\n",
    "        # Стоп-слова: удаление стоп-слов - это стандартная практика очистки текстов\n",
    "    stop_words = set(stopwords.words('russian'))\n",
    "    tokens = word_tokenize(clean_text)\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    clean_text = ' '.join(tokens)\n",
    "\n",
    "    # Знаки препинания: далее - удаляем из текста все знаки препинания\n",
    "    clean_text = re.sub(r'[^\\w\\s]', '', clean_text)\n",
    "\n",
    "    # И наконец - возвращаем очищенный текст\n",
    "    return clean_text\n",
    "\n",
    "# Функция для преобразования эмоджи в слова\n",
    "def emojis_words(text):\n",
    "\n",
    "    # Модуль emoji: преобразование эмоджи в их словесные описания\n",
    "    clean_text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "\n",
    "    # Редактирование текста путём замены \":\" и\" _\", а так же - путём добавления пробела между отдельными словами\n",
    "    clean_text = clean_text.replace(\":\", \"\").replace(\"_\", \" \")\n",
    "\n",
    "    return clean_text\n",
    "\n",
    "def preproc_text(text):\n",
    "    res = str(text).strip()\n",
    "    res = re.sub(r\"\\s+\", \" \", res)\n",
    "    return res\n",
    "\n",
    "def build_data(data_q, data_ans):\n",
    "    data = []\n",
    "    for idx, texts in enumerate(data_q):\n",
    "        question = preproc_text(texts)\n",
    "        answer = preproc_text(data_ans.iloc[idx])\n",
    "        res = '\\nx:' + question + '\\ny:' + answer\n",
    "        data.append(res)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmap\n",
    "import re\n",
    "\n",
    "def get_num_lines(file_path):\n",
    "    fp = open(file_path, \"r+\")\n",
    "    buf = mmap.mmap(fp.fileno(), 0)\n",
    "    lines = 0\n",
    "    while buf.readline():\n",
    "        lines += 1\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c9ef8da615e4123afa26fb5338bbcb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1163342 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Обработка текста\n",
    "\n",
    "sentences = []\n",
    "morpher = MorphAnalyzer()\n",
    "sw = set(get_stop_words(\"ru\"))\n",
    "exclude = set(string.punctuation)\n",
    "exclude = {}\n",
    "c = 0\n",
    "\n",
    "file_path_from = 'prepared_answers.txt'\n",
    "file_path_to = 'Otvety2.txt'\n",
    "\n",
    "if not os.path.isfile(file_path_to):\n",
    "    \n",
    "    N = get_num_lines(file_path_from)\n",
    "    with open(file_path_to, mode = 'w', encoding=\"utf8\") as fileto:\n",
    "        with open(file_path_from, encoding=\"utf8\") as filefrom:\n",
    "            for k in tqdm(range(N)):\n",
    "                line = filefrom.readline()\n",
    "                if line == '': break\n",
    "                spls = preprocess_txt(line)\n",
    "                sentences.append(spls)\n",
    "                c += 1\n",
    "                if c > 50_000:\n",
    "                    break\n",
    "                fileto.write(' '.join(spls)+'\\n')\n",
    "    filefrom.close()\n",
    "    fileto.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c2415587ff4477cb96a18e8faeec51b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Загрузить результат\n",
    "\n",
    "sentences = []\n",
    "\n",
    "file_path_from = 'Otvety2.txt'\n",
    "if os.path.isfile(file_path_from):  \n",
    "    N = get_num_lines(file_path_from) \n",
    "    with open(file_path_to, mode = 'r', encoding=\"utf8\") as filefrom:\n",
    "        for k in tqdm(range(N)):\n",
    "            line = filefrom.readline()\n",
    "            if line == '': break\n",
    "            sentences.append(line.split())\n",
    "    filefrom.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['%',\n",
       " '(',\n",
       " '(+)',\n",
       " '(205—239),',\n",
       " '(28—75)',\n",
       " '(299—325)',\n",
       " '(439—472)',\n",
       " '(452—498)',\n",
       " '(534—560)',\n",
       " '(5строк)']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = []\n",
    "_ = [vec.extend(x)  for x in sentences[:100]]\n",
    "vec = list(set(vec))\n",
    "vec.sort()\n",
    "vec[20:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучим модель FastText\n",
    "\n",
    "file_path_from = 'ft_model'\n",
    "if not os.path.isfile(file_path_from):  \n",
    "    \n",
    "    sentences = [i for i in tqdm(sentences) if len(i) > 2]\n",
    "    modelFT = FastText(sentences=sentences, size=100, min_count=1, window=5)\n",
    "    modelFT.save(\"ft_model\")\n",
    "    ft_index = annoy.AnnoyIndex(100 ,'angular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузить модель\n",
    "\n",
    "modelFT = FastText.load(\"ft_model\")\n",
    "ft_index = annoy.AnnoyIndex(100 ,'angular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "937693e3301f4631af2668663703b36c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:10, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_path_from = 'speaker.ann'\n",
    "morpher = MorphAnalyzer()\n",
    "sw = set(get_stop_words(\"ru\"))\n",
    "exclude = set(string.punctuation)\n",
    "modelFT = FastText.load(\"ft_model\")\n",
    "ft_index = annoy.AnnoyIndex(100 ,'angular')\n",
    "\n",
    "index_map = {}\n",
    "counter = 0\n",
    "with open(\"Otvety2.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "    for line in tqdm(f):\n",
    "        n_ft = 0\n",
    "        spls = line.split(\"===\")\n",
    "        index_map[counter] = spls[1]\n",
    "        question = preprocess_txt(spls[0])\n",
    "        vector_ft = np.zeros(100)\n",
    "        for word in question:\n",
    "            if word in modelFT.wv:\n",
    "                vector_ft += modelFT.wv[word]\n",
    "                n_ft += 1\n",
    "        if n_ft > 0:\n",
    "            vector_ft = vector_ft / n_ft\n",
    "        ft_index.add_item(counter, vector_ft)\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "        if counter > 50_000:\n",
    "            break\n",
    "\n",
    "ft_index.build(10)\n",
    "ft_index.save('speaker.ann')\n",
    "    \n",
    "with open(\"index_speaker.pkl\", \"wb\") as f:\n",
    "    pickle.dump(index_map, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Загрузим индексы\n",
    "ft_index = annoy.AnnoyIndex(100, 'angular')\n",
    "ft_index.load('speaker.ann')\n",
    "index_map = pd.read_pickle(\"index_speaker.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32f9373375f848d69007f0e904458e82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>descrirption</th>\n",
       "      <th>product_id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>subcategory_id</th>\n",
       "      <th>properties</th>\n",
       "      <th>image_links</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Юбка детская ORBY</td>\n",
       "      <td>Новая, не носили ни разу. В реале красивей чем...</td>\n",
       "      <td>58e3cfe6132ca50e053f5f82</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2211</td>\n",
       "      <td>{'detskie_razmer_rost': '81-86 (1,5 года)'}</td>\n",
       "      <td>http://cache3.youla.io/files/images/360_360/58...</td>\n",
       "      <td>[юбка, детский, orby, новый, носить, реал, кра...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ботильоны</td>\n",
       "      <td>Новые,привезены из Чехии ,указан размер 40,но ...</td>\n",
       "      <td>5667531b2b7f8d127d838c34</td>\n",
       "      <td>9.0</td>\n",
       "      <td>902</td>\n",
       "      <td>{'zhenskaya_odezhda_tzvet': 'Зеленый', 'visota...</td>\n",
       "      <td>http://cache3.youla.io/files/images/360_360/5b...</td>\n",
       "      <td>[ботильон, новыепривезти, чехия, указать, разм...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Брюки</td>\n",
       "      <td>Размер 40-42. Брюки почти новые - не знаю как ...</td>\n",
       "      <td>59534826aaab284cba337e06</td>\n",
       "      <td>9.0</td>\n",
       "      <td>906</td>\n",
       "      <td>{'zhenskaya_odezhda_dzhinsy_bryuki_tip': 'Брюк...</td>\n",
       "      <td>http://cache3.youla.io/files/images/360_360/59...</td>\n",
       "      <td>[брюки, размер, 4042, брюки, новый, знать, мер...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Продам детские шапки</td>\n",
       "      <td>Продам шапки,кажда 200р.Розовая и белая проданны.</td>\n",
       "      <td>57de544096ad842e26de8027</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2217</td>\n",
       "      <td>{'detskie_pol': 'Девочкам', 'detskaya_odezhda_...</td>\n",
       "      <td>http://cache3.youla.io/files/images/360_360/57...</td>\n",
       "      <td>[продать, детский, шапка, продать, шапкикажда,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Блузка</td>\n",
       "      <td>Темно-синяя, 42 размер,состояние отличное,как ...</td>\n",
       "      <td>5ad4d2626c86cb168d212022</td>\n",
       "      <td>9.0</td>\n",
       "      <td>907</td>\n",
       "      <td>{'zhenskaya_odezhda_tzvet': 'Синий', 'zhenskay...</td>\n",
       "      <td>http://cache3.youla.io/files/images/360_360/5a...</td>\n",
       "      <td>[блузка, темносиний, 42, размерсостояние, отли...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  title                                       descrirption  \\\n",
       "0     Юбка детская ORBY  Новая, не носили ни разу. В реале красивей чем...   \n",
       "1             Ботильоны  Новые,привезены из Чехии ,указан размер 40,но ...   \n",
       "2                 Брюки  Размер 40-42. Брюки почти новые - не знаю как ...   \n",
       "3  Продам детские шапки  Продам шапки,кажда 200р.Розовая и белая проданны.   \n",
       "4                Блузка  Темно-синяя, 42 размер,состояние отличное,как ...   \n",
       "\n",
       "                 product_id  category_id subcategory_id  \\\n",
       "0  58e3cfe6132ca50e053f5f82         22.0           2211   \n",
       "1  5667531b2b7f8d127d838c34          9.0            902   \n",
       "2  59534826aaab284cba337e06          9.0            906   \n",
       "3  57de544096ad842e26de8027         22.0           2217   \n",
       "4  5ad4d2626c86cb168d212022          9.0            907   \n",
       "\n",
       "                                          properties  \\\n",
       "0        {'detskie_razmer_rost': '81-86 (1,5 года)'}   \n",
       "1  {'zhenskaya_odezhda_tzvet': 'Зеленый', 'visota...   \n",
       "2  {'zhenskaya_odezhda_dzhinsy_bryuki_tip': 'Брюк...   \n",
       "3  {'detskie_pol': 'Девочкам', 'detskaya_odezhda_...   \n",
       "4  {'zhenskaya_odezhda_tzvet': 'Синий', 'zhenskay...   \n",
       "\n",
       "                                         image_links  \\\n",
       "0  http://cache3.youla.io/files/images/360_360/58...   \n",
       "1  http://cache3.youla.io/files/images/360_360/5b...   \n",
       "2  http://cache3.youla.io/files/images/360_360/59...   \n",
       "3  http://cache3.youla.io/files/images/360_360/57...   \n",
       "4  http://cache3.youla.io/files/images/360_360/5a...   \n",
       "\n",
       "                                                text  \n",
       "0  [юбка, детский, orby, новый, носить, реал, кра...  \n",
       "1  [ботильон, новыепривезти, чехия, указать, разм...  \n",
       "2  [брюки, размер, 4042, брюки, новый, знать, мер...  \n",
       "3  [продать, детский, шапка, продать, шапкикажда,...  \n",
       "4  [блузка, темносиний, 42, размерсостояние, отли...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создадим модель продуктовых данных\n",
    "\n",
    "shop_data = pd.read_csv(\"ProductsDataset.csv\")\n",
    "#shop_data = shop_data.iloc[:5000, :]\n",
    "\n",
    "shop_data['text'] = shop_data['title'] + \" \" + shop_data[\"descrirption\"]\n",
    "shop_data['text'] = shop_data['text'].progress_apply(lambda x: preprocess_txt(str(x)))\n",
    "shop_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовка для создания модели для определения домена данных\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "297d86c0cda744249f2d0f97ac4a60ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25357 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "987efc7013eb468bb8088e2df6f99c9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idxs = set(np.random.randint(0, len(index_map), len(shop_data)))\n",
    "# Вопрос-ответный домен\n",
    "negative_texts = [\" \".join(preprocess_txt(index_map[i])) for i in tqdm(idxs)]\n",
    "# Продуктовый домен\n",
    "positive_texts = [\" \".join(val) for val in tqdm(shop_data['text'].values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ВО = 0, Прод = 1\n",
    "\n",
    "dataset = negative_texts + positive_texts\n",
    "labels = np.zeros(len(dataset))\n",
    "labels[len(negative_texts):] = np.ones(len(positive_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset, labels, test_size=0.2, \n",
    "                                                    stratify=labels, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\virt_dev\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Модель\n",
    "\n",
    "x_train_vec = vectorizer.fit_transform(X_train)\n",
    "x_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "lr = LogisticRegression().fit(x_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9778343321566374"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Качество\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_true=y_test, y_pred=lr.predict(x_test_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfs = {v[0]: v[1] for v in zip(tfidf_vect.vocabulary_, tfidf_vect.idf_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35670e65df56420e8e9541c511dae238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Создаем Индексы для продуктовых данных\n",
    "\n",
    "file_path_from = 'shop.ann'\n",
    "if not os.path.isfile(file_path_from):  \n",
    "    \n",
    "    \n",
    "    ft_index_shop = annoy.AnnoyIndex(100 ,'angular')\n",
    "\n",
    "    midf = np.mean(tfidf_vect.idf_)\n",
    "\n",
    "    index_map_shop = {}\n",
    "    counter = 0\n",
    "\n",
    "    for i in tqdm(range(len(shop_data))):\n",
    "        n_ft = 0\n",
    "        index_map_shop[counter] = (shop_data.loc[i, \"title\"], shop_data.loc[i, \"image_links\"])\n",
    "        vector_ft = np.zeros(100)\n",
    "        for word in shop_data.loc[i, \"text\"]:\n",
    "            if word in modelFT.wv:\n",
    "                vector_ft += modelFT.wv[word] * idfs.get(word, midf)\n",
    "                n_ft += idfs.get(word, midf)\n",
    "        if n_ft > 0:\n",
    "            vector_ft = vector_ft / n_ft\n",
    "        ft_index_shop.add_item(counter, vector_ft)\n",
    "        counter += 1\n",
    "\n",
    "    ft_index_shop.build(10)\n",
    "    ft_index_shop.save('shop.ann')\n",
    "\n",
    "    file_path_from = 'index_shop.pkl'\n",
    "    if not os.path.isfile(file_path_from):  \n",
    "    \n",
    "        with open(\"index_shop.pkl\", \"wb\") as f:\n",
    "            pickle.dump(index_map_shop, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузим индексы\n",
    "\n",
    "midf = np.mean(tfidf_vect.idf_)\n",
    "\n",
    "ft_index_shop = annoy.AnnoyIndex(100, 'angular')\n",
    "ft_index_shop.load('shop.ann') \n",
    "\n",
    "index_map_shop = pd.read_pickle(\"index_shop.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Основная функция преобразования текста в вектор х100\n",
    "\n",
    "def embed_txt(txt, idfs, midf):\n",
    "    n_ft = 0\n",
    "    vector_ft = np.zeros(100)\n",
    "    for word in txt:\n",
    "        if word in modelFT.wv:\n",
    "            vector_ft += modelFT.wv[word] * idfs.get(word, midf)\n",
    "            n_ft += idfs.get(word, midf)\n",
    "    return vector_ft / n_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Пользователь\\AppData\\Local\\Temp\\ipykernel_15312\\2571992157.py:40: TelegramDeprecationWarning: The argument `clean` of `start_polling` is deprecated. Please use `drop_pending_updates` instead.\n",
      "  updater.start_polling(clean=True)\n",
      "No error handlers are registered, logging exception.\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\virt_dev\\.venv\\lib\\site-packages\\telegram\\vendor\\ptb_urllib3\\urllib3\\connectionpool.py\", line 402, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 2, in raise_from\n",
      "  File \"d:\\virt_dev\\.venv\\lib\\site-packages\\telegram\\vendor\\ptb_urllib3\\urllib3\\connectionpool.py\", line 398, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"d:\\python_3.9\\lib\\http\\client.py\", line 1347, in getresponse\n",
      "    response.begin()\n",
      "  File \"d:\\python_3.9\\lib\\http\\client.py\", line 307, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"d:\\python_3.9\\lib\\http\\client.py\", line 268, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"d:\\python_3.9\\lib\\socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"d:\\python_3.9\\lib\\ssl.py\", line 1241, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"d:\\python_3.9\\lib\\ssl.py\", line 1099, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\virt_dev\\.venv\\lib\\site-packages\\telegram\\utils\\request.py\", line 259, in _request_wrapper\n",
      "    resp = self._con_pool.request(*args, **kwargs)\n",
      "  File \"d:\\virt_dev\\.venv\\lib\\site-packages\\telegram\\vendor\\ptb_urllib3\\urllib3\\request.py\", line 68, in request\n",
      "    return self.request_encode_body(method, url, fields=fields,\n",
      "  File \"d:\\virt_dev\\.venv\\lib\\site-packages\\telegram\\vendor\\ptb_urllib3\\urllib3\\request.py\", line 148, in request_encode_body\n",
      "    return self.urlopen(method, url, **extra_kw)\n",
      "  File \"d:\\virt_dev\\.venv\\lib\\site-packages\\telegram\\vendor\\ptb_urllib3\\urllib3\\poolmanager.py\", line 244, in urlopen\n",
      "    response = conn.urlopen(method, u.request_uri, **kw)\n",
      "  File \"d:\\virt_dev\\.venv\\lib\\site-packages\\telegram\\vendor\\ptb_urllib3\\urllib3\\connectionpool.py\", line 665, in urlopen\n",
      "    retries = retries.increment(method, url, error=e, _pool=self,\n",
      "  File \"d:\\virt_dev\\.venv\\lib\\site-packages\\telegram\\vendor\\ptb_urllib3\\urllib3\\util\\retry.py\", line 347, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"d:\\virt_dev\\.venv\\lib\\site-packages\\telegram\\vendor\\ptb_urllib3\\urllib3\\packages\\six.py\", line 686, in reraise\n",
      "    raise value\n",
      "  File \"d:\\virt_dev\\.venv\\lib\\site-packages\\telegram\\vendor\\ptb_urllib3\\urllib3\\connectionpool.py\", line 614, in urlopen\n",
      "    httplib_response = self._make_request(conn, method, url,\n",
      "  File \"d:\\virt_dev\\.venv\\lib\\site-packages\\telegram\\vendor\\ptb_urllib3\\urllib3\\connectionpool.py\", line 404, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout,\n",
      "  File \"d:\\virt_dev\\.venv\\lib\\site-packages\\telegram\\vendor\\ptb_urllib3\\urllib3\\connectionpool.py\", line 321, in _raise_timeout\n",
      "    raise exc_cls(*args)\n",
      "telegram.vendor.ptb_urllib3.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='api.telegram.org', port=443): Read timed out. (read timeout=5.0)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\virt_dev\\.venv\\lib\\site-packages\\telegram\\ext\\dispatcher.py\", line 557, in process_update\n",
      "    handler.handle_update(update, self, check, context)\n",
      "  File \"d:\\virt_dev\\.venv\\lib\\site-packages\\telegram\\ext\\handler.py\", line 199, in handle_update\n",
      "    return self.callback(update, context)\n",
      "  File \"C:\\Users\\Пользователь\\AppData\\Local\\Temp\\ipykernel_15312\\2571992157.py\", line 34, in textMessage\n",
      "    context.bot.send_message(chat_id=update.message.chat_id, text=index_map[ft_index_val[0]])\n",
      "  File \"d:\\virt_dev\\.venv\\lib\\site-packages\\telegram\\bot.py\", line 134, in decorator\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"d:\\virt_dev\\.venv\\lib\\site-packages\\telegram\\bot.py\", line 534, in send_message\n",
      "    return self._message(  # type: ignore[return-value]\n",
      "  File \"d:\\virt_dev\\.venv\\lib\\site-packages\\telegram\\ext\\extbot.py\", line 203, in _message\n",
      "    result = super()._message(\n",
      "  File \"d:\\virt_dev\\.venv\\lib\\site-packages\\telegram\\bot.py\", line 344, in _message\n",
      "    result = self._post(endpoint, data, timeout=timeout, api_kwargs=api_kwargs)\n",
      "  File \"d:\\virt_dev\\.venv\\lib\\site-packages\\telegram\\bot.py\", line 299, in _post\n",
      "    return self.request.post(\n",
      "  File \"d:\\virt_dev\\.venv\\lib\\site-packages\\telegram\\utils\\request.py\", line 361, in post\n",
      "    result = self._request_wrapper(\n",
      "  File \"d:\\virt_dev\\.venv\\lib\\site-packages\\telegram\\utils\\request.py\", line 261, in _request_wrapper\n",
      "    raise TimedOut() from error\n",
      "telegram.error.TimedOut: Timed out\n"
     ]
    }
   ],
   "source": [
    "\n",
    "updater = Updater(\"6728330934:AAGuAYeHKl9WqdWno_NC9b_eXZoM4KaCf8Q\", use_context=True) # Токен API к Telegram\n",
    "dispatcher = updater.dispatcher\n",
    "\n",
    "def startCommand(update, context):\n",
    "    context.bot.send_message(chat_id=update.message.chat_id, text='Привет, давай пообщаемся?')\n",
    "\n",
    "def textMessage(update, context):\n",
    "    \n",
    "    input_txt = preprocess_txt(update.message.text)\n",
    "    vect = vectorizer.transform([\" \".join(input_txt)])\n",
    "    prediction = lr.predict(vect)\n",
    "    \n",
    "    if prediction[0] == 1:\n",
    "        vect_ft = embed_txt(input_txt, idfs, midf)\n",
    "        ft_index_shop_val = ft_index_shop.get_nns_by_vector(vect_ft, 5)\n",
    "        for item in ft_index_shop_val:\n",
    "            title, image = index_map_shop[item]\n",
    "            context.bot.send_message(chat_id=update.message.chat_id, text=\"title: {} image: {}\".format(title, image))\n",
    "        return\n",
    "    \n",
    "    vect_ft = embed_txt(input_txt, {}, 1)\n",
    "    ft_index_val, distances = ft_index.get_nns_by_vector(vect_ft, 1, include_distances=True)\n",
    "    \n",
    "    # \n",
    "    if distances[0] > 100.5:\n",
    "        print(distances[0])\n",
    "        context.bot.send_message(chat_id=update.message.chat_id, text=\"Моя твоя не понимать\")\n",
    "        return\n",
    "    \n",
    "    context.bot.send_message(chat_id=update.message.chat_id, text=index_map[ft_index_val[0]])\n",
    "        \n",
    "start_command_handler = CommandHandler('start', startCommand)\n",
    "text_message_handler = MessageHandler(Filters.text, textMessage)\n",
    "dispatcher.add_handler(start_command_handler)\n",
    "dispatcher.add_handler(text_message_handler)\n",
    "updater.start_polling(clean=True)\n",
    "updater.idle()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
