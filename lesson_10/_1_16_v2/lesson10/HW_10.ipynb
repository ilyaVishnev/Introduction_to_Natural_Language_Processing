{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tnxXKDjq3jEL"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kRVATYOgJs1b"
      },
      "outputs": [],
      "source": [
        "# Download the file\n",
        "path_to_file = \"deu-eng/deu.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rd0jw-eC3jEh"
      },
      "outputs": [],
      "source": [
        "def preprocess_sentence(w):\n",
        "  w = w.lower().strip()\n",
        "\n",
        "  # creating a space between a word and the punctuation following it\n",
        "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "  w = re.sub(r\"([?.!,])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "  w = re.sub(r\"[^a-zA-Zа-яА-Я?.!,']+\", \" \", w)\n",
        "\n",
        "  w = w.strip()\n",
        "\n",
        "  # adding a start and an end token to the sentence\n",
        "  # so that the model know when to start and stop predicting.\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "yV9lZXQXNbnH",
        "outputId": "94d4631a-3aab-4c70-86dd-3fafe55d9bd5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<start> iсh cann gehen . <end>'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preprocess_sentence(\"Iсh cann gehen.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OHn4Dct23jEm"
      },
      "outputs": [],
      "source": [
        "# 1. Remove the accents\n",
        "# 2. Clean the sentences\n",
        "# 3. Return word pairs in the format: [ENG, RUS]\n",
        "def create_dataset(path, num_examples):\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')[:2]]  for l in lines[:num_examples]]\n",
        "\n",
        "  return zip(*word_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTbSbBz55QtF",
        "outputId": "f4afb29e-3c2c-4514-8ca7-3a53e59a2149"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<start> go . <end>\n",
            "<start> geh . <end>\n"
          ]
        }
      ],
      "source": [
        "en, de = create_dataset(path_to_file, None)\n",
        "print(en[0])\n",
        "print(de[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bIOn8RCNDJXG"
      },
      "outputs": [],
      "source": [
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "eAY9k49G3jE_"
      },
      "outputs": [],
      "source": [
        "def load_dataset(path, num_examples=None):\n",
        "  # creating cleaned input, output pairs\n",
        "  targ_lang, inp_lang = create_dataset(path, num_examples)\n",
        "\n",
        "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOi42V79Ydlr"
      },
      "source": [
        "### Limit the size of the dataset to experiment faster (optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8j9g9AnIeZV",
        "outputId": "2636bf07-32de-40d8-eae2-1c134e7a3b9e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(267186, 267186)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(en), len(de)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cnxC7q-j3jFD"
      },
      "outputs": [],
      "source": [
        "# Try experimenting with the size of that dataset\n",
        "num_examples = 100000\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QILQkOs3jFG",
        "outputId": "ea3e3574-eec6-43a7-8883-b6294cccc87c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "80000 80000 20000 20000\n"
          ]
        }
      ],
      "source": [
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "lJPmLZGMeD5q"
      },
      "outputs": [],
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXukARTDd7MT",
        "outputId": "4ddf254e-b67a-4d5a-b34e-16ad63377580"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> <start>\n",
            "5 ----> tom\n",
            "7 ----> ist\n",
            "55 ----> sein\n",
            "389 ----> bruder\n",
            "3 ----> .\n",
            "2 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "4 ----> tom\n",
            "8 ----> is\n",
            "51 ----> his\n",
            "360 ----> brother\n",
            "3 ----> .\n",
            "2 ----> <end>\n"
          ]
        }
      ],
      "source": [
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgCLkfv5uO3d"
      },
      "source": [
        "### Create a tf.data dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "TqHsArVZ3jFS"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 300\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc6-NK1GtWQt",
        "outputId": "f34f644b-c3d5-4a5d-f7a9-7587a9b21c1b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(TensorShape([64, 23]), TensorShape([64, 12]))"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68zZxRZCveW5"
      },
      "source": [
        "**Для эксперимента возьму двунаправленный слой GRU**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "x3EfyzA6glQm"
      },
      "outputs": [],
      "source": [
        "from keras.layers import GRU, Concatenate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "nZ2rI24i3jFg"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.Bidirectional(GRU(self.enc_units,\n",
        "                                   return_sequences=False,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform'))\n",
        "    \n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state_frw, state_bcw = self.gru(x, initial_state = hidden)\n",
        "    return state_frw, state_bcw\n",
        "\n",
        "  # def initialize_hidden_state(self):\n",
        "  #   return tf.zeros((self.batch_sz, self.enc_units))\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "      return [tf.zeros((self.batch_sz, self.enc_units)) for i in range(2)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKHA_t1KkZkV",
        "outputId": "ecc45dc6-5d8b-4ebb-8c48-579f35262a4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoder forward_h shape: (batch size, units) (64, 1024)\n",
            "Encoder backward_h shape: (batch size, units) (64, 1024)\n"
          ]
        }
      ],
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "# sample input\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "forward_hidden, backward_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print('Encoder forward_h shape: (batch size, units) {}'.format(forward_hidden.shape))\n",
        "print('Encoder backward_h shape: (batch size, units) {}'.format(backward_hidden.shape))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "yJ_B3mhW3jFk"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(2*self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, x, encoder_fwd_state, encoder_back_state):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x, initial_state=Concatenate(axis=-1)([encoder_fwd_state, encoder_back_state]))\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "P5UY8wko3jFp"
      },
      "outputs": [],
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "decoder_sample_x, decoder_sample_h = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      forward_hidden, backward_hidden)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKcypC0AGeLR",
        "outputId": "c8116e1c-731e-4c62-91c3-0b7553a567e4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([64, 8856])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "decoder_sample_x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6y0HF-zMF_vp",
        "outputId": "27f4771f-005f-4147-ef4f-3dbe54f13cb0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([64, 2048])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "decoder_sample_h.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ch_71VbIRfK"
      },
      "source": [
        "## Define the optimizer and the loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "WmTHr5iV3jFr"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMVWzzsfNl4e"
      },
      "source": [
        "## Checkpoints (Object-based saving)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Zj8bXQTgNwrF"
      },
      "outputs": [],
      "source": [
        "checkpoint_dir = './training_nmt_checkpoints'\n",
        "\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "sC9ArXSsVfqn"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    forward_hidden, backward_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]): # Максимальная длина последовательности \n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden = decoder(dec_input, forward_hidden, backward_hidden)\n",
        " \n",
        "      # считаем лосс по каждому предсказанному слову\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddefjBMa3jF0",
        "outputId": "84eafe29-6a33-4e2c-e444-626ad27794aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 0.7478\n",
            "Epoch 1 Batch 100 Loss 0.6565\n",
            "Epoch 1 Batch 200 Loss 0.6743\n",
            "Epoch 1 Batch 300 Loss 0.6723\n",
            "Epoch 1 Batch 400 Loss 0.6334\n",
            "Epoch 1 Batch 500 Loss 0.6998\n",
            "Epoch 1 Batch 600 Loss 0.6579\n",
            "Epoch 1 Batch 700 Loss 0.5940\n",
            "Epoch 1 Batch 800 Loss 0.6211\n",
            "Epoch 1 Batch 900 Loss 0.6283\n",
            "Epoch 1 Batch 1000 Loss 0.5943\n",
            "Epoch 1 Batch 1100 Loss 0.6217\n",
            "Epoch 1 Batch 1200 Loss 0.5487\n",
            "Epoch 1 Loss 0.6285\n",
            "Time taken for 1 epoch 27072.61021232605 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.3416\n",
            "Epoch 2 Batch 100 Loss 0.2788\n",
            "Epoch 2 Batch 200 Loss 0.4407\n",
            "Epoch 2 Batch 300 Loss 0.3663\n",
            "Epoch 2 Batch 400 Loss 0.2925\n",
            "Epoch 2 Batch 500 Loss 0.3427\n",
            "Epoch 2 Batch 600 Loss 0.3143\n",
            "Epoch 2 Batch 700 Loss 0.3458\n",
            "Epoch 2 Batch 800 Loss 0.3806\n",
            "Epoch 2 Batch 900 Loss 0.3674\n",
            "Epoch 2 Batch 1000 Loss 0.4050\n",
            "Epoch 2 Batch 1100 Loss 0.3507\n",
            "Epoch 2 Batch 1200 Loss 0.3230\n",
            "Epoch 2 Loss 0.3523\n",
            "Time taken for 1 epoch 25183.27890229225 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.1757\n",
            "Epoch 3 Batch 100 Loss 0.1364\n",
            "Epoch 3 Batch 200 Loss 0.1586\n",
            "Epoch 3 Batch 300 Loss 0.1909\n",
            "Epoch 3 Batch 400 Loss 0.2521\n",
            "Epoch 3 Batch 500 Loss 0.2227\n",
            "Epoch 3 Batch 600 Loss 0.2288\n",
            "Epoch 3 Batch 700 Loss 0.1756\n",
            "Epoch 3 Batch 800 Loss 0.1932\n",
            "Epoch 3 Batch 900 Loss 0.1982\n",
            "Epoch 3 Batch 1000 Loss 0.2217\n",
            "Epoch 3 Batch 1100 Loss 0.2036\n",
            "Epoch 3 Batch 1200 Loss 0.2181\n",
            "Epoch 3 Loss 0.2229\n",
            "Time taken for 1 epoch 24694.443491220474 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.1165\n",
            "Epoch 4 Batch 100 Loss 0.1480\n",
            "Epoch 4 Batch 200 Loss 0.1471\n",
            "Epoch 4 Batch 300 Loss 0.1096\n",
            "Epoch 4 Batch 400 Loss 0.1256\n",
            "Epoch 4 Batch 500 Loss 0.1544\n",
            "Epoch 4 Batch 600 Loss 0.2163\n",
            "Epoch 4 Batch 700 Loss 0.2544\n",
            "Epoch 4 Batch 800 Loss 0.1842\n",
            "Epoch 4 Batch 900 Loss 0.1995\n",
            "Epoch 4 Batch 1000 Loss 0.1926\n",
            "Epoch 4 Batch 1100 Loss 0.1432\n",
            "Epoch 4 Batch 1200 Loss 0.2025\n",
            "Epoch 4 Loss 0.1680\n",
            "Time taken for 1 epoch 27064.059873342514 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.0949\n",
            "Epoch 5 Batch 100 Loss 0.1061\n",
            "Epoch 5 Batch 200 Loss 0.1121\n",
            "Epoch 5 Batch 300 Loss 0.1824\n",
            "Epoch 5 Batch 400 Loss 0.1192\n",
            "Epoch 5 Batch 500 Loss 0.1521\n",
            "Epoch 5 Batch 600 Loss 0.1542\n",
            "Epoch 5 Batch 700 Loss 0.1050\n",
            "Epoch 5 Batch 800 Loss 0.1351\n",
            "Epoch 5 Batch 900 Loss 0.1290\n",
            "Epoch 5 Batch 1000 Loss 0.1510\n",
            "Epoch 5 Batch 1100 Loss 0.1512\n",
            "Epoch 5 Batch 1200 Loss 0.1641\n",
            "Epoch 5 Loss 0.1392\n",
            "Time taken for 1 epoch 24766.158593654633 sec\n",
            "\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "EbQpyYs13jF_"
      },
      "outputs": [],
      "source": [
        "def evaluate(sentence):\n",
        "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units)) for i in range(2)]\n",
        "  enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  forward_hidden, backward_hidden = enc_hidden\n",
        "\n",
        "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "  for t in range(max_length_targ):\n",
        "    predictions, dec_hidden = decoder(dec_input, forward_hidden, backward_hidden)\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "    result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "    if targ_lang.index_word[predicted_id] == '<end>':\n",
        "      return result, sentence\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "sl9zUHzg3jGI"
      },
      "outputs": [],
      "source": [
        "def translate(sentence):\n",
        "  result, sentence = evaluate(sentence)\n",
        "\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n250XbnjOaqP"
      },
      "source": [
        "## Restore the latest checkpoint and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJpT9D5_OgP6",
        "outputId": "473f4e1a-6877-4fbc-cc66-c1c47a0cfc29"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f64b241c3a0>"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "# checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrAM0FDomq3E",
        "outputId": "59ed9dfc-cdc7-4085-902e-bc7e132db72c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input: <start> das ist <end>\n",
            "Predicted translation: that's upsetting . <end> \n"
          ]
        }
      ],
      "source": [
        "translate('Das ist ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bhFfwcIMX5i",
        "outputId": "5332e5f5-90df-401b-ce3b-3718e985dfd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input: <start> ich kann nicht gehen <end>\n",
            "Predicted translation: i can't go . <end> \n"
          ]
        }
      ],
      "source": [
        "translate('Ich kann nicht gehen')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSx2iM36EZQZ",
        "outputId": "56d285b7-9990-4aa8-8a53-6b05ec89d7bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input: <start> sind sie zu hause ? <end>\n",
            "Predicted translation: are you home ? <end> \n"
          ]
        }
      ],
      "source": [
        "translate(u'Sind sie zu hause?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3LLCx3ZE0Ls",
        "outputId": "c47e6f0e-d37f-45c9-8f9a-cc994e2efbe0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input: <start> ich liebe dich <end>\n",
            "Predicted translation: i love you . <end> \n"
          ]
        }
      ],
      "source": [
        "translate(u'Ich liebe dich')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUQVLVqUE1YW",
        "outputId": "ae952952-d5f2-4418-8cfc-a0d0cc3c991b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input: <start> es schneit <end>\n",
            "Predicted translation: it's snowing . <end> \n"
          ]
        }
      ],
      "source": [
        "translate(u'Es schneit')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
