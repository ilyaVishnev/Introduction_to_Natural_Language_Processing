{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 736,
     "status": "ok",
     "timestamp": 1674986894541,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "5R9e3pctHIV2",
    "outputId": "96c0bd11-e9b0-40de-f8b1-5c1b05149299"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size = 25000\n",
      "Test size = 25000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"train.tsv\", delimiter=\"\\t\")\n",
    "test_df = pd.read_csv(\"test.tsv\", delimiter=\"\\t\")\n",
    "\n",
    "print('Train size = {}'.format(len(train_df)))\n",
    "print('Test size = {}'.format(len(test_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6oHbJCPPwhjp"
   },
   "source": [
    "Посмотрите глазами на тексты? Какие есть зацепки, как определить, что это за сентимент?\n",
    "\n",
    "Самое простое, как всегда - найти ключевые слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 569,
     "status": "ok",
     "timestamp": 1674987242234,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "DywUCyMLr_TD",
    "outputId": "b63a231b-c75d-42a4-a893-33e7d6a95085"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy = 66.73%\n"
     ]
    }
   ],
   "source": [
    "#@title Начинаем классифицировать! { vertical-output: true, display-mode: \"form\" }\n",
    "positive_words = 'love', 'great', 'best', 'wonderful' #@param {type:\"raw\"}\n",
    "negative_words = 'worst', 'awful', '1/10', 'crap' #@param {type:\"raw\"}\n",
    "\n",
    "positives_count = test_df.review.apply(lambda text: sum(word in text for word in positive_words))\n",
    "negatives_count = test_df.review.apply(lambda text: sum(word in text for word in negative_words))\n",
    "is_positive = positives_count > negatives_count\n",
    "correct_count = (is_positive == test_df.is_positive).values.sum()\n",
    "\n",
    "accuracy = correct_count / len(test_df)\n",
    "\n",
    "print('Test accuracy = {:.2%}'.format(accuracy))\n",
    "if accuracy > 0.71:\n",
    "    from IPython.display import Image, display\n",
    "    display(Image('https://s3.amazonaws.com/achgen360/t/rmmoZsub.png', width=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oo8HRABxv1kW"
   },
   "source": [
    "**Задание** Придумайте хорошие ключевые слова или фразы и наберите хотя бы 71% точности на тесте (и не забудьте посмотреть на код классификации!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hqa9iD18PuX5"
   },
   "source": [
    "Решил воспользоваться читерским методом: \"посмотрел на несколько ячеек с кодом вперед\". И добавил к спискам позитивных и негативных слов те, которые будут в будущем выявлены с помощью обработки датасета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2912,
     "status": "ok",
     "timestamp": 1674991128139,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "PFRa64QOpvKi",
    "outputId": "55f81726-445f-4149-f12a-720ab99ccf70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct count:18931, len(test_df):25000.\n",
      "Test accuracy = 75.72%\n"
     ]
    }
   ],
   "source": [
    "positive_words = ('love', 'great', 'best', '7/10', '810', 'refreshing', 'wonderfully', 'funniest',\n",
    "                  'erotic', 'excellent', 'carrey', 'flawless', 'hooked',\n",
    "                  'superb', 'perfect', 'subtle', 'highly', 'vengeance',\n",
    "                  'incredible', 'surprisingly', 'perfectly', 'rare', 'driven',\n",
    "                  'finest', 'australia', 'touching', 'favorite', 'appreciated',\n",
    "                  'chavez', 'enjoyable', 'scariest', '9/10', 'underrated',\n",
    "                  '10/10', 'amazing', 'steals', 'tears', 'whoopi', 'delightful',\n",
    "                  'flight', 'squirrel', 'units', 'kitty', 'enjoyed',\n",
    "                  'dealing', 'bound')\n",
    "\n",
    "\n",
    "negative_words = ('4/10', 'disappointment', '3/10', 'waste', 'poorly', 'worst', 'lacks',\n",
    "                   '2/10','laughable', 'fails', 'awful', 'avoid', 'boring',\n",
    "                   'disappointing', 'forgettable', '2/10', 'alright', 'unfunny',\n",
    "                   'pointless', 'mediocre', 'lame', 'save', 'lousy', 'mst3k',\n",
    "                   'horrible', 'badly', 'wooden', 'weak', 'baldwin', 'redeeming',\n",
    "                   'mildly', 'dull', 'worse', 'mess', 'dreadful', 'britney',\n",
    "                   '1/10', 'annoying', 'unwatchable', 'photo', 'poor', 'generous',\n",
    "                   'sources', 'terrible', 'bored', 'boredom', 'insult', 'wonder',\n",
    "                   'uninteresting', 'ridiculous', 'cardboard', 'unfortunately',\n",
    "                   'miscast', 'ludicrous', 'refer', 'hoping', 'obnoxious', 'basically')\n",
    "\n",
    "\n",
    "positives_count = test_df.review.apply(lambda text: sum(word in text for word in positive_words))\n",
    "negatives_count = test_df.review.apply(lambda text: sum(word in text for word in negative_words))\n",
    "\n",
    "is_positive = positives_count > negatives_count\n",
    "\n",
    "correct_count = (is_positive == test_df.is_positive).values.sum()\n",
    "print(f'correct count:{correct_count}, len(test_df):{len(test_df)}.')\n",
    "\n",
    "accuracy = correct_count / len(test_df)\n",
    "print('Test accuracy = {:.2%}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eaIrBClMUHZB"
   },
   "source": [
    "**Задание** Кому-нибудь нравятся эти `<br /><br />`? Лично мне - нет. Напишите регулярку, которая будет их удалять"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 283,
     "status": "ok",
     "timestamp": 1674986646723,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "09OkUmtde6ny",
    "outputId": "f67b354e-4259-404a-9768-a7a06a7360b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This show comes up with interesting locations as fast as the travel channel. It is billed as reality but in actuality it is pure prime time soap opera. It's tries to use exotic locales as a facade to bring people into a phony contest & then proceeds to hook viewers on the contestants soap opera style.<br /><br />It also borrows from an early CBS game show pioneer- Beat The Clock- by inventing situations for its contestants to try & overcome. Then it rewards the winner money. If they can spice it up with a little interaction between the characters, even better. While the game format is in slow motion versus Beat The Clock- the real accomplishment of this series is to escape reality. <br /><br />This show has elements of several types of successful past programs. Reality television, hardly, but if your hooked on the contestants, locale or contest, this is your cup of tea. If your not, this entire series is as I say, drivel dripping with gravy. It is another show hiding behind the reality label which is the trend it started in 2000.<br /><br />It is slick & well produced, so it might last a while yet. After all, so do re-runs of Gilligan's Island, Green Acres, The Beverly Hillbillies & The Brady Bunch. This just doesn't employ professional actors. The intelligence level is about the same.\n",
      "------------------\n",
      "This show comes up with interesting locations as fast as the travel channel. It is billed as reality but in actuality it is pure prime time soap opera. It's tries to use exotic locales as a facade to bring people into a phony contest & then proceeds to hook viewers on the contestants soap opera style.  It also borrows from an early CBS game show pioneer- Beat The Clock- by inventing situations for its contestants to try & overcome. Then it rewards the winner money. If they can spice it up with a little interaction between the characters, even better. While the game format is in slow motion versus Beat The Clock- the real accomplishment of this series is to escape reality.   This show has elements of several types of successful past programs. Reality television, hardly, but if your hooked on the contestants, locale or contest, this is your cup of tea. If your not, this entire series is as I say, drivel dripping with gravy. It is another show hiding behind the reality label which is the trend it started in 2000.  It is slick & well produced, so it might last a while yet. After all, so do re-runs of Gilligan's Island, Green Acres, The Beverly Hillbillies & The Brady Bunch. This just doesn't employ professional actors. The intelligence level is about the same.\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern = re.compile('<br\\s?\\/>|<br>')\n",
    "\n",
    "print(train_df['review'].iloc[1])\n",
    "print('------------------')\n",
    "\n",
    "print(pattern.subn(' ', train_df['review'].iloc[1])[0])\n",
    "print('------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vO6D9NuMi4II"
   },
   "source": [
    "Применим ее:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 290,
     "status": "ok",
     "timestamp": 1674986649095,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "7LTwQqs_hD-K"
   },
   "outputs": [],
   "source": [
    "train_df['review'] = train_df['review'].apply(lambda text: pattern.subn(' ', text)[0])\n",
    "test_df['review'] = test_df['review'].apply(lambda text: pattern.subn(' ', text)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1yPbQcpraO2"
   },
   "source": [
    "### Сделаем дополнительную предобработку текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2624,
     "status": "ok",
     "timestamp": 1674986652872,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "Wgw9c7uDrsWa",
    "outputId": "28ee9e3e-7800-4ee2-ee74-6b6976c2ea4e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Пользователь\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 265,
     "status": "ok",
     "timestamp": 1674986654342,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "zqhU19a8rUwX"
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "mystopwords = stopwords.words(\"english\") + ['the', 'a']\n",
    "\n",
    "def remove_punktuation(text):\n",
    "    return re.sub(r'[^\\w\\s\\d]', '', text)\n",
    "\n",
    "def lower_case(text):\n",
    "    text = str(text).lower()\n",
    "    return ' '.join(tokenizer.tokenize(text))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = tokenizer.tokenize(text)\n",
    "    text = [w for w in text if w not in mystopwords]\n",
    "    return ' '.join(text)\n",
    "    \n",
    "def normalize(text):\n",
    "    text = remove_punktuation(text)\n",
    "    text = lower_case(text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 19587,
     "status": "ok",
     "timestamp": 1674986676042,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "L-tevpuOrhqy"
   },
   "outputs": [],
   "source": [
    "train_df['review'] = train_df['review'].apply(normalize)\n",
    "test_df['review'] = test_df['review'].apply(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vGxzf4oXmXqw"
   },
   "source": [
    "Пора переходить к машинке!\n",
    "\n",
    "Как будем представлять текст? Проще всего - мешком слов.\n",
    "\n",
    "Заведём большой-большой словарь - список всех слов в обучающей выборке. Тогда каждое предложение можно представить в виде вектора, в котором будет записано, сколько раз встретилось каждое из возможных слов:\n",
    "\n",
    "![bow](https://raw.githubusercontent.com/DanAnastasyev/DeepNLP-Course/master/Week%2001/Images/BOW.png)\n",
    "\n",
    "Простой и приятный способ сделать это - запихнуть тексты в `CountVectorizer`.\n",
    "\n",
    "Он имеет такую сигнатуру:\n",
    "\n",
    "```python\n",
    "CountVectorizer(input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern=r'(?u)\\b\\w\\w+\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class ‘numpy.int64'>)\n",
    "```\n",
    "\n",
    "Для начала обратим внимание на параметры `lowercase=True` и `max_df=1.0, min_df=1, max_features=None` - они про то, что по умолчанию все слова будут приводиться к нижнему регистру и в словарь попадут все слова, встречавшиеся в текстах.\n",
    "\n",
    "При желании можно было бы убрать слишком редкие или слишком частотные слова - пока не будем этого делать.\n",
    "\n",
    "Посмотрим на простом примере, как он будет работать:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 253,
     "status": "ok",
     "timestamp": 1674986683522,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "5Odnum4iyGDr",
    "outputId": "d2dcb9f6-ab49-4e57-bc0b-4487087d7075"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 1]\n",
      " [1 0 1 1 1]]\n",
      "['awful' 'excellent' 'movie' 'the' 'was']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "dummy_data = ['The movie was excellent',\n",
    "              'the movie was awful']\n",
    "\n",
    "dummy_matrix = vectorizer.fit_transform(dummy_data)\n",
    "\n",
    "print(dummy_matrix.toarray())\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3zC1ItWybVc"
   },
   "source": [
    "*Как именно vectorizer определяет границы слов? Обратите внимание на параметр `token_pattern=r'(?u)\\b\\w\\w+\\b'` - как он будет работать?*\n",
    "\n",
    "Запустим его на реальных данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2445,
     "status": "ok",
     "timestamp": 1674986688224,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "Ccd2gaCdQq2W",
    "outputId": "ca08383f-29d7-441a-a0a3-9670eb97effb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(train_df['review'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_JC9n6C0bFR"
   },
   "source": [
    "Посмотрим на слова, попавшие в словарь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 258,
     "status": "ok",
     "timestamp": 1674986689883,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "stV4ICO3mKsf",
    "outputId": "b05f34f7-190c-4502-ba0b-ba68fa57fdbb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '000', '0000000000001', ..., 'überannoying', 'überspy',\n",
       "       'üvegtigris'], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oesbuQ9g0krj"
   },
   "source": [
    "Попробуем кого-нибудь таки сконвертировать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 244,
     "status": "ok",
     "timestamp": 1674986692969,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "lUWtDWcp0g7U",
    "outputId": "48b58452-a62b-451f-9bb7-1572fcfd3ec5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x111517 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 157 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform([train_df['review'].iloc[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZCRef3pyCRC"
   },
   "source": [
    "То, что и хотели - вектор с bow (т.е. bag-of-words) представлением исходного текста.\n",
    "\n",
    "И чем эта информация может помочь? Ну, всё тем же - какие-то слова носят положительный окрас, какие-то - отрицательный. Большинство вообще нейтральный, да.\n",
    "\n",
    "![bow with weights](https://github.com/DanAnastasyev/DeepNLP-Course/raw/master/Week%2001/Images/BOW_weights.png)\n",
    "\n",
    "Хочется, наверное, подобрать коэффициенты, которые будут определять уровень окраса, да? Подбирать нужно по обучающей выборке, а не как мы перед этим делали.\n",
    "\n",
    "Например, для выборки\n",
    "```\n",
    "1   The movie was excellent\n",
    "0   the movie was awful\n",
    "```\n",
    "легко подобрать коэффициенты на глазок: что-нибудь вроде `+1` для `excellent`,  `-1` для `awful` и по нулям всем остальным.\n",
    "\n",
    "Построим линейную модель, которая станет этим заниматься. Она будет учиться строить разделяющую гиперплоскость в пространстве bow-векторов.\n",
    "\n",
    "Проверим, как справится логистическая регрессия с нашей супер-выборкой из пары предложений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 375,
     "status": "ok",
     "timestamp": 1674986696578,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "i6WVgK4LtUn2",
    "outputId": "c0eb49fb-2397-453b-e75d-70b9bce9645a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['awful' 'excellent' 'movie' 'the' 'was']\n",
      "[[-0.40104279  0.40104279  0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "dummy_data = ['The movie was excellent',\n",
    "              'the movie was awful']\n",
    "dummy_labels = [1, 0]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "model = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "model.fit(dummy_data, dummy_labels)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(classifier.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C9Y-kq-tv-XY"
   },
   "source": [
    "Получилось что надо.\n",
    "\n",
    "Запустим теперь её на реальных данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10443,
     "status": "ok",
     "timestamp": 1674986709797,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "kvxHIIbSiUXq",
    "outputId": "e5c758f1-9c7c-450d-fca4-00fa38f27b02"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\virt_dev_4\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;, CountVectorizer()),\n",
       "                (&#x27;classifier&#x27;, LogisticRegression())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;, CountVectorizer()),\n",
       "                (&#x27;classifier&#x27;, LogisticRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('vectorizer', CountVectorizer()),\n",
       "                ('classifier', LogisticRegression())])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_df['review'], train_df['is_positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2182,
     "status": "ok",
     "timestamp": 1674986714113,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "-d3BBV_uUu-O",
    "outputId": "60dbf9ce-89f7-415d-977a-fe526822d554"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy = 86.99%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def eval_model(model, test_df):\n",
    "    preds = model.predict(test_df['review'])\n",
    "    print('Test accuracy = {:.2%}'.format(accuracy_score(test_df['is_positive'], preds)))\n",
    "    \n",
    "eval_model(model, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 396
    },
    "executionInfo": {
     "elapsed": 275,
     "status": "ok",
     "timestamp": 1674991155111,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "v-4sVWBOWJpo",
    "outputId": "b0479db1-ca5b-492f-e01b-26f2ce7d12bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "print('Positive' if test_df['is_positive'].iloc[1] else 'Negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "executionInfo": {
     "elapsed": 241,
     "status": "ok",
     "timestamp": 1674991158369,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "AoZhtlYlW-xG",
    "outputId": "18815ab8-87c7-4942-e6e8-738d7ffe5daf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "print('Positive' if test_df['is_positive'].iloc[6] else 'Negative')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNNUqZvplhAC"
   },
   "source": [
    "Посмотрим на примеры неправильной классификации, наконец:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 572
    },
    "executionInfo": {
     "elapsed": 3859,
     "status": "ok",
     "timestamp": 1674991164979,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "yf9ZzS8fXKFm",
    "outputId": "27096078-c85d-4c9a-a229-1ab8d75c8906"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "preds_vount_vect = model.predict(test_df['review'])\n",
    "\n",
    "incorrect_pred_index = np.random.choice(np.where(preds_vount_vect != test_df['is_positive'])[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZbFXKNrngP46"
   },
   "source": [
    "## Придумываем новые признаки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dz7GSFzIlv4V"
   },
   "source": [
    "### Tf-idf\n",
    "\n",
    "Сейчас мы на все слова смотрим с одинаковым весом - хотя какие-то из них более редкие, какие-то более частые, и эта частотность - полезная, вообще говоря, информация.\n",
    "\n",
    "Самый простой способ добавить статистическую информацию о частотностях - сделать *tf-idf* взвешивание:\n",
    "\n",
    "$$\\text{tf-idf}(t, d) = \\text{tf}(t, d) \\times \\text{idf}(t)$$\n",
    "\n",
    "*tf* - term-frequency - частотность слова `t` в конкретном документе `d` (рецензии в нашем случае). Это ровно то, что мы уже считали.\n",
    "\n",
    "*idf* - inverse document-frequency - коэффициент, который тем больше, чем в меньшем числе документов встречалось данное слово. Считается как-нибудь так:\n",
    "$$\\text{idf}(t) = \\text{log}\\frac{1 + n_d}{1 + n_{d(t)}} + 1$$\n",
    "где $n_d$ - число всех документов, а $n_{d(t)}$ - число документов со словом `t`.\n",
    "\n",
    "Использовать его просто - нужно заменить `CountVectorizer` на `TfidfVectorizer`.\n",
    "\n",
    "**Задание** Попробуйте запустить `TfidfVectorizer`. Посмотрите на ошибки, которые он научился исправлять, и на ошибки, которые он начал делать - по сравнению с `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10560,
     "status": "ok",
     "timestamp": 1674991183135,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "p3DjjiJglvT3",
    "outputId": "855bfd00-2dc1-46ee-d3c4-dc39133d477c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy = 88.58%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "model = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "model.fit(train_df['review'], train_df['is_positive'])\n",
    "\n",
    "eval_model(model, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 396
    },
    "executionInfo": {
     "elapsed": 249,
     "status": "ok",
     "timestamp": 1674991210763,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "tzvE5QbSsZBT",
    "outputId": "d16483bb-96c1-426d-e90d-9711d8cc6213"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "print('Positive' if test_df['is_positive'].iloc[1] else 'Negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "executionInfo": {
     "elapsed": 989,
     "status": "ok",
     "timestamp": 1674991213389,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "-lBhU07bsblE",
    "outputId": "b6e5a69b-f067-4bf3-8c46-c0a5713a8de7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "print('Positive' if test_df['is_positive'].iloc[6] else 'Negative')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xe_CJxQ0tFP9"
   },
   "source": [
    "### N-граммы слов\n",
    "\n",
    "До сих пор мы смотрели на тексты как на мешок слов - но очевидно, что есть разница между `good movie` и `not good movie`.\n",
    "\n",
    "Добавим информацию (хоть какую-то) о последовательностях слов - будем извлекать еще и биграммы слов.\n",
    "\n",
    "В Vectorizer'ах для этого есть параметр `ngram_range=(n_1, n_2)` - он говорит, что нужны n_1-...n_2-граммы.\n",
    "\n",
    "**Задание** Попробуйте увеличенный range и поинтерпретируйте полученный результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49268,
     "status": "ok",
     "timestamp": 1674991265197,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "RDpdrT0HuKYN",
    "outputId": "ef57e330-e608-4d68-82bc-063982b3b148"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy = 88.28%\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "model = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "model.fit(train_df['review'], train_df['is_positive'])\n",
    "\n",
    "eval_model(model, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 145112,
     "status": "ok",
     "timestamp": 1674991414966,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "R2kPrQbbxQsy",
    "outputId": "71451d7c-8bfc-4461-8a8f-b9c6096a4dd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy = 87.55%\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3))\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "model = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "model.fit(train_df['review'], train_df['is_positive'])\n",
    "\n",
    "eval_model(model, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 240812,
     "status": "ok",
     "timestamp": 1674991656701,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "D41_wPKHxcHy",
    "outputId": "45ef25d5-9559-40e6-f4f4-6d0016bece47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy = 86.62%\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 5))\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "model = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "model.fit(train_df['review'], train_df['is_positive'])\n",
    "\n",
    "eval_model(model, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YrBoThj6wl2F"
   },
   "source": [
    "### N-граммы символов\n",
    "\n",
    "Символьные n-граммы дают простой способ выучить полезные корни и суффиксы, не связываясь с этой вашей лингвистикой - только статистика, только хардкор.\n",
    "\n",
    "Например, слово `badass` мы можем представить в виде такой последовательности триграмм:\n",
    "\n",
    "`##b #ba bad ada das ass ss# s##`\n",
    "\n",
    "So interpretable, неправда ли?\n",
    "\n",
    "Реализовать это дело всё так же просто - нужно поставить `analyzer='char'` в вашем любимом Vectorizer'е и выбрать размер `ngram_range`.\n",
    "\n",
    "**Задание** Запилите классификатор на n-граммах символов и визуализируйте его."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 248359,
     "status": "ok",
     "timestamp": 1674991920303,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "QFaWmUrGyY3n",
    "outputId": "d4163bfd-8c8e-4b33-e3b1-17054e2d1a2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy = 87.56%\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(2, 6), max_features=20000, analyzer='char')\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "model = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "model.fit(train_df['review'], train_df['is_positive'])\n",
    "\n",
    "eval_model(model, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "executionInfo": {
     "elapsed": 524,
     "status": "ok",
     "timestamp": 1674991920809,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "9E1E1Bn8yvhq",
    "outputId": "4ed12444-a571-477d-f11b-9d922d1558d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "print('Positive' if test_df['is_positive'].iloc[1] else 'Negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 532096,
     "status": "ok",
     "timestamp": 1674992457612,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "fXrLEKzlzsLv",
    "outputId": "60d0ac3c-ec9b-4d54-f76c-d7fc3ffaca09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy = 88.33%\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(3, 7), analyzer='char')\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "model = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "model.fit(train_df['review'], train_df['is_positive'])\n",
    "\n",
    "eval_model(model, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "executionInfo": {
     "elapsed": 9406,
     "status": "ok",
     "timestamp": 1674992467001,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "dFkFOkBiz8yR",
    "outputId": "34d16af8-5218-494e-93e2-5a422428401d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "print('Positive' if test_df['is_positive'].iloc[1] else 'Negative')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9fZ6I8mN0VPU"
   },
   "source": [
    "## Подключаем лингвистику"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YkywIvbp4N-L"
   },
   "source": [
    "### Лемматизация и стемминг\n",
    "\n",
    "Если присмотреться, можно найти формы одного слова с разной семантической окраской по мнению классификатора. Или нет?\n",
    "\n",
    "**Задание** Найти формы слова с разной семантической окраской.\n",
    "\n",
    "Поверя, что они есть, попробуем что-нибудь с этим сделать.\n",
    "\n",
    "Например, лемматизируем - сведем к начальной форме все слова. Поможет в этом библиотека spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "executionInfo": {
     "elapsed": 3182,
     "status": "ok",
     "timestamp": 1674992521227,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "2SJs-rSkRuvk"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser'])\n",
    "\n",
    "docs = [doc for doc in nlp.pipe(train_df.review.values[:50])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1098,
     "status": "ok",
     "timestamp": 1674992525750,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "xoxdm8JrR0wG",
    "outputId": "c16f9f09-c0ca-45a3-f73c-84e43c0eecdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dreamgirls dreamgirl B ORG\n",
      "despite despite O \n",
      "fistful fistful O \n",
      "tony tony O \n",
      "wins win O \n",
      "incredibly incredibly O \n",
      "weak weak O \n",
      "year year B DATE\n",
      "broadway broadway O \n",
      "never never O \n",
      "one one O \n",
      "would would O \n",
      "call call O \n",
      "jewel jewel B PERSON\n",
      "crown crown I PERSON\n",
      "stage stage O \n",
      "musicals musical O \n",
      "however however O \n",
      "say say O \n",
      "right right O \n",
      "cinematic cinematic O \n",
      "hands hand O \n",
      "could could O \n",
      "fleshed fleshed O \n",
      "polished polished O \n",
      "something something O \n",
      "worthwhile worthwhile O \n",
      "onscreen onscreen O \n",
      "unfortunately unfortunately O \n",
      "transfers transfer O \n",
      "screen screen O \n",
      "basically basically O \n",
      "slavishly slavishly O \n",
      "faithful faithful O \n",
      "version version O \n",
      "stage stage O \n",
      "hit hit O \n",
      "inherent inherent O \n",
      "weaknesses weakness O \n",
      "intact intact O \n",
      "first first B ORDINAL\n",
      "score score O \n",
      "never never O \n",
      "one one O \n",
      "strong strong O \n",
      "points point O \n",
      "production production O \n",
      "film film O \n",
      "change change O \n",
      "factor factor O \n",
      "lots lot O \n",
      "songs song O \n",
      "perhaps perhaps O \n",
      "many many O \n",
      "especially especially O \n",
      "memorable memorable O \n",
      "closest close O \n",
      "come come O \n",
      "catchy catchy O \n",
      "tunes tune O \n",
      "title title O \n",
      "song song O \n",
      "one one B TIME\n",
      "night night I TIME\n",
      "much much O \n",
      "acclaimed acclaimed O \n",
      "telling telling O \n",
      "going go O \n",
      "less less O \n",
      "great great O \n",
      "song song O \n",
      "dramatic dramatic O \n",
      "set set O \n",
      "piece piece O \n",
      "character character O \n",
      "effie effie O \n",
      "jennifer jennifer B PERSON\n",
      "hudson hudson I PERSON\n",
      "film film O \n",
      "slick slick O \n",
      "technically technically O \n",
      "wellproduced wellproduced O \n",
      "story story O \n",
      "characters character O \n",
      "surprisingly surprisingly O \n",
      "thin thin O \n",
      "lacking lack O \n",
      "resonance resonance O \n",
      "interest interest O \n",
      "opening opening O \n",
      "moments moment O \n",
      "watching watch O \n",
      "jamie jamie B PERSON\n",
      "foxxs foxxs I PERSON\n",
      "svengalilike svengalilike O \n",
      "manager manager O \n",
      "manipulate manipulate O \n",
      "acts act O \n",
      "top top O \n",
      "takes take O \n",
      "back back O \n",
      "seat seat O \n",
      "latter latter O \n",
      "portion portion O \n",
      "film film O \n",
      "story story O \n",
      "conveniently conveniently O \n",
      "tries try O \n",
      "cast cast O \n",
      "villain villain O \n",
      "despite despite O \n",
      "right right O \n",
      "business business O \n",
      "standpoint standpoint O \n",
      "good good O \n",
      "majority majority O \n",
      "film film O \n",
      "beyonce beyonce O \n",
      "knowles knowle O \n",
      "lovely lovely O \n",
      "sings sing O \n",
      "songs song O \n",
      "perfectly perfectly O \n",
      "well well O \n",
      "stuck stuck O \n",
      "character character O \n",
      "basically basically O \n",
      "surface surface O \n",
      "glitz glitz O \n",
      "anika anika O \n",
      "noni noni B PERSON\n",
      "rose rise O \n",
      "third third B ORDINAL\n",
      "member member O \n",
      "dreamgirls dreamgirls B PERSON\n",
      "trio trio I PERSON\n",
      "literally literally O \n",
      "nothing nothing O \n",
      "entire entire O \n",
      "film film O \n",
      "eddie eddie O \n",
      "murphy murphy B PERSON\n",
      "acquits acquit O \n",
      "well well O \n",
      "singer singer O \n",
      "obviously obviously O \n",
      "based base O \n",
      "james james B PERSON\n",
      "brown brown I PERSON\n",
      "role role O \n",
      "especially especially O \n",
      "meaty meaty O \n",
      "ultimately ultimately O \n",
      "little little O \n",
      "impact impact O \n",
      "foxx foxx O \n",
      "would would O \n",
      "seem seem O \n",
      "ideal ideal O \n",
      "casting casting O \n",
      "seems seem O \n",
      "oddly oddly O \n",
      "withdrawn withdraw O \n",
      "bored bored O \n",
      "films film O \n",
      "biggest big O \n",
      "selling selling O \n",
      "point point O \n",
      "surely surely O \n",
      "former former O \n",
      "american american B ORG\n",
      "idol idol I ORG\n",
      "contestantoscar contestantoscar O \n",
      "winner winner O \n",
      "jennifer jennifer B PERSON\n",
      "hudson hudson I PERSON\n",
      "central central O \n",
      "role role O \n",
      "effie effie O \n",
      "white white O \n",
      "temperamental temperamental O \n",
      "singer singer O \n",
      "gets get O \n",
      "booted boot O \n",
      "group group O \n",
      "makes make O \n",
      "triumphant triumphant O \n",
      "closing closing O \n",
      "act act O \n",
      "return return O \n",
      "effie effie B ORG\n",
      "always always O \n",
      "big big O \n",
      "problem problem O \n",
      "show show O \n",
      "movie movie O \n",
      "film film O \n",
      "obviously obviously O \n",
      "wants want O \n",
      "feel feel O \n",
      "sorry sorry O \n",
      "rather rather O \n",
      "hamhandedly hamhandedly O \n",
      "takes take O \n",
      "side side O \n",
      "never never O \n",
      "sure sure O \n",
      "character character O \n",
      "deserves deserve O \n",
      "kind kind O \n",
      "devotion devotion O \n",
      "start start O \n",
      "effie effie B ORG\n",
      "conducts conduct O \n",
      "part part O \n",
      "like like O \n",
      "obnoxious obnoxious O \n",
      "egotistical egotistical O \n",
      "selfcentered selfcentered O \n",
      "diva diva O \n",
      "interested interested O \n",
      "everyone everyone O \n",
      "else else O \n",
      "rather rather O \n",
      "much much O \n",
      "vested vested O \n",
      "interest interest O \n",
      "group group O \n",
      "part part O \n",
      "booted boot O \n",
      "group group O \n",
      "unprofessionalism unprofessionalism O \n",
      "bad bad O \n",
      "attitude attitude O \n",
      "charges charge O \n",
      "wellfounded wellfounde O \n",
      "stage stage O \n",
      "showfilm showfilm O \n",
      "seem seem O \n",
      "think think O \n",
      "effie effie B ORG\n",
      "cut cut O \n",
      "unlimited unlimited O \n",
      "slack slack O \n",
      "simply simply O \n",
      "great great O \n",
      "voice voice O \n",
      "even even O \n",
      "though though O \n",
      "film film O \n",
      "tries try O \n",
      "soften soften O \n",
      "effies effie O \n",
      "harder hard O \n",
      "edges edge O \n",
      "make make O \n",
      "likable likable O \n",
      "charges charge O \n",
      "still still O \n",
      "stand stand O \n",
      "story story O \n",
      "becomes become O \n",
      "manipulative manipulative O \n",
      "suggesting suggest O \n",
      "sympathy sympathy O \n",
      "unwed unwed O \n",
      "mother mother O \n",
      "struggling struggle O \n",
      "raise raise O \n",
      "daughter daughter O \n",
      "using use O \n",
      "implication implication O \n",
      "much much O \n",
      "like like O \n",
      "talent talent O \n",
      "card card O \n",
      "motherhood motherhood O \n",
      "immediately immediately O \n",
      "makes make O \n",
      "behavior behavior O \n",
      "excusable excusable O \n",
      "indeed indeed O \n",
      "big big O \n",
      "effort effort O \n",
      "film film O \n",
      "makes make O \n",
      "show show O \n",
      "effies effie O \n",
      "mothering mothering O \n",
      "tell tell O \n",
      "us we O \n",
      "include include O \n",
      "scene scene O \n",
      "barks barks O \n",
      "daughter daughter O \n",
      "unemployment unemployment O \n",
      "office office O \n",
      "insists insist O \n",
      "girl girl O \n",
      "father father O \n",
      "refuse refuse O \n",
      "look look O \n",
      "gainful gainful O \n",
      "employment employment O \n",
      "support support O \n",
      "since since O \n",
      "singing singing O \n",
      "knows know O \n",
      "hands hand O \n",
      "skillful skillful O \n",
      "actress actress O \n",
      "gaps gap O \n",
      "could could O \n",
      "perhaps perhaps O \n",
      "remedied remedied O \n",
      "technique technique O \n",
      "charisma charisma O \n",
      "unfortunately unfortunately O \n",
      "hudson hudson O \n",
      "actress actress O \n",
      "sings sing O \n",
      "well well O \n",
      "dialogdriven dialogdriven O \n",
      "moments moment O \n",
      "come come O \n",
      "naturally naturally O \n",
      "high high O \n",
      "emotional emotional O \n",
      "moments moment O \n",
      "effies effie O \n",
      "signature signature O \n",
      "moment moment O \n",
      "aforementioned aforementione O \n",
      "telling tell O \n",
      "number number O \n",
      "wellsung wellsung O \n",
      "hudson hudson O \n",
      "emotionally emotionally O \n",
      "flat flat O \n",
      "acting act O \n",
      "department department O \n",
      "effie effie B ORG\n",
      "supposed suppose O \n",
      "expressing express O \n",
      "rage rage O \n",
      "desperation desperation O \n",
      "predicament predicament O \n",
      "hudson hudson O \n",
      "comes come O \n",
      "cabaret cabaret O \n",
      "performer performer O \n",
      "belting belt O \n",
      "hot hot O \n",
      "number number O \n",
      "quite quite O \n",
      "emotional emotional O \n",
      "highlight highlight O \n",
      "one one O \n",
      "expects expect O \n",
      "latter latter O \n",
      "portion portion O \n",
      "film film O \n",
      "basically basically O \n",
      "predictable predictable O \n",
      "melange melange O \n",
      "events event O \n",
      "maneuver maneuver O \n",
      "foxx foxx O \n",
      "hudsons hudson O \n",
      "earlier early O \n",
      "position position O \n",
      "allow allow O \n",
      "strut strut O \n",
      "back back O \n",
      "lord lord O \n",
      "everyone everyone O \n",
      "foxxs foxxs O \n",
      "criminal criminal O \n",
      "offenses offenses O \n",
      "film film O \n",
      "undoubtedly undoubtedly O \n",
      "par par O \n",
      "course course O \n",
      "many many O \n",
      "struggling struggle O \n",
      "record record O \n",
      "producers producer O \n",
      "films film O \n",
      "seeming seem O \n",
      "implication implication O \n",
      "coming come O \n",
      "helped help O \n",
      "usher usher O \n",
      "disco disco O \n",
      "era era O \n",
      "rather rather O \n",
      "ridiculous ridiculous O \n",
      "mention mention O \n",
      "pretentious pretentious O \n",
      "condescending condescending O \n",
      "particularly particularly O \n",
      "coming come O \n",
      "film film O \n",
      "depth depth O \n",
      "puddle puddle O \n",
      "end end O \n",
      "result result O \n",
      "faithful faithful O \n",
      "rendition rendition O \n",
      "stage stage O \n",
      "hit hit O \n",
      "drained drain O \n",
      "emotion emotion O \n",
      "energy energy O \n",
      "anything anything O \n",
      "described describe O \n",
      "dynamic dynamic O \n"
     ]
    }
   ],
   "source": [
    "for token in docs[0]:\n",
    "    print(token.text, token.lemma_, token.ent_iob_, token.ent_type_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Pfup3O5r30m"
   },
   "source": [
    "**Задание** Сделайте классификатор на лемматизированных текстах.\n",
    "\n",
    "Более простой способ нормализации слов - использовать стемминг. Он немного тупой, не учитывает контекст, но иногда оказывается даже эффективнее лемматизации - а, главное, быстрее.\n",
    "\n",
    "По сути это просто набор правил, как обрезать слово, чтобы получить основу (stem):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 227,
     "status": "ok",
     "timestamp": 1674992570025,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "YvZt4TiS4LmQ",
    "outputId": "f4c51d4f-596f-4cb2-cf98-740d2b0ea3a1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Пользователь\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "executionInfo": {
     "elapsed": 285,
     "status": "ok",
     "timestamp": 1674992572534,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "zKDIVgmm4NZ6"
   },
   "outputs": [],
   "source": [
    "def lemmatized_words(text):\n",
    "    text_lem = [lemmatizer.lemmatize(word) for word in tokenizer.tokenize(text)]\n",
    "    return ' '.join(word for word in text_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 240,
     "status": "ok",
     "timestamp": 1674992575305,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "M3RRrABrSBex",
    "outputId": "de3b7239-738e-4c49-d455-e2b42b9a11bf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Пользователь\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "executionInfo": {
     "elapsed": 47061,
     "status": "ok",
     "timestamp": 1674992624536,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "fmVv55l94ZQw"
   },
   "outputs": [],
   "source": [
    "train_df['lemmatized'] = train_df['review'].apply(lemmatized_words)\n",
    "test_df['lemmatized'] = test_df['review'].apply(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47368,
     "status": "ok",
     "timestamp": 1674992706088,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "Mp8wW47-4a-I",
    "outputId": "cb3162c7-ef4a-48c6-fa19-b01c872329a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy = 87.97%\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "model = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "model.fit(train_df['lemmatized'], train_df['is_positive'])\n",
    "\n",
    "eval_model(model, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NxGSEqHNsfHy"
   },
   "source": [
    "**Задание** Попробуйте вместо лемм классифицировать основы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "executionInfo": {
     "elapsed": 229,
     "status": "ok",
     "timestamp": 1674992708719,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "kNKdYgNj5rxP"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "executionInfo": {
     "elapsed": 369,
     "status": "ok",
     "timestamp": 1674992710994,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "kU_MbnqR5tff"
   },
   "outputs": [],
   "source": [
    "def stemmed_words(text):\n",
    "    text_stem = [stemmer.stem(word) for word in tokenizer.tokenize(text)]\n",
    "    return ' '.join(word for word in text_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "executionInfo": {
     "elapsed": 172774,
     "status": "ok",
     "timestamp": 1674992884809,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "3NPLFnJl55jH"
   },
   "outputs": [],
   "source": [
    "train_df['stemmed'] = train_df['review'].apply(stemmed_words)\n",
    "test_df['stemmed'] = test_df['review'].apply(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 38448,
     "status": "ok",
     "timestamp": 1674992923249,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "aslrzDhz568T",
    "outputId": "ddedb363-441b-46a8-b4f8-a3fcb392bda8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy = 82.37%\n"
     ]
    }
   ],
   "source": [
    "model = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "model.fit(train_df['stemmed'], train_df['is_positive'])\n",
    "\n",
    "eval_model(model, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYTPNoBN_bGu"
   },
   "source": [
    "Score выше при использовании лемм"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2fHA70b0zEZ"
   },
   "source": [
    "## Включаем дип лёрнинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1VCrM671XO5"
   },
   "source": [
    "Мы тут пришли deep learning'ом заниматься, а делаем почему-то модель на логистической регрессии. Как так?\n",
    "\n",
    "Попробуем запустить относительно стандартную модель для классификации текстов - сверточная сеть поверх словных эмбеддингов.\n",
    "\n",
    "Разбираться, что это за зверь, будем на следующих занятиях, а пока будем просто им пользоваться :)\n",
    "\n",
    "Каждое предложение нужно представлять набором слов - и сразу же начинаются проблемы. Во-первых, как ограничить длину предложения?\n",
    "\n",
    "Прикинем по гистограмме, какая длина нам подходит:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1674992923250,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "02Kv8YUPbhzG",
    "outputId": "78fdc2aa-9e2e-406a-8a41-e722a563fa39"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        dreamgirls despite fistful tony wins incredibl...\n",
       "1        show comes interesting locations fast travel c...\n",
       "2        simply love movie also love ramones sorta bias...\n",
       "3        spoilers ahead want call would almost recommen...\n",
       "4        alltime favorite movie seen many movies one be...\n",
       "                               ...                        \n",
       "24995    big fan movie usual reasons think travolta win...\n",
       "24996    im going bother plot synopsis since know movie...\n",
       "24997    movie dont know would take indellible characte...\n",
       "24998    saw film dvd yesterday gobsmacked flabbergaste...\n",
       "24999    disappointment none nuance original brits seem...\n",
       "Name: review, Length: 25000, dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "executionInfo": {
     "elapsed": 899,
     "status": "ok",
     "timestamp": 1674992924132,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "O477ZV1t1WIO",
    "outputId": "7ff12cf2-58d3-4ca0-9278-94dcd77a8d5f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 253 artists>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_, _, hist = plt.hist(train_df.review.apply(lambda text: len(text.split())), bins='auto')\n",
    "hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXO4xi0u5m8l"
   },
   "source": [
    "Кроме этого, нужно перенумеровать как-то слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1221,
     "status": "ok",
     "timestamp": 1674992925348,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "mIMGE7L-55fs",
    "outputId": "8e2c5a32-d5c5-48cf-fb5f-eae15a8f91a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words count 20441\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "words_counter = Counter((word for text in train_df.review for word in text.lower().split()))\n",
    "\n",
    "word2idx = {\n",
    "    '': 0,\n",
    "    '<unk>': 1\n",
    "}\n",
    "for word, count in words_counter.most_common():\n",
    "    if count < 10:\n",
    "        break\n",
    "        \n",
    "    word2idx[word] = len(word2idx)\n",
    "    \n",
    "print('Words count', len(word2idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oTptlmd1yD3J"
   },
   "source": [
    "**Задание** Сконвертируйте данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6194,
     "status": "ok",
     "timestamp": 1674992931540,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "ZPP0cYdJ5VkE",
    "outputId": "b82dd8ac-9aa7-44f1-f991-0d65fd06a396"
   },
   "outputs": [],
   "source": [
    "def convert(texts, word2idx, max_text_len):\n",
    "    data = np.zeros((len(texts), max_text_len), dtype=int)#np.int)\n",
    "    \n",
    "    for inx, text in enumerate(texts):\n",
    "        result = []\n",
    "        for word in text.split():\n",
    "            if word in word2idx:\n",
    "                result.append(word2idx[word])\n",
    "        padding = [0]*(max_text_len - len(result))\n",
    "        data[inx] = np.array(padding + result[-max_text_len:], dtype=int)#np.int)\n",
    "    return data\n",
    "\n",
    "X_train = convert(train_df.review, word2idx, 1000)\n",
    "X_test = convert(test_df.review, word2idx, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYb-p5ioyLUZ"
   },
   "source": [
    "Поставим учиться модельку на keras.\n",
    "\n",
    "*Напоминание*: на keras, чтобы обучить модель, нужно\n",
    "1. Определить модель, например:\n",
    "```python \n",
    "model = Sequential()\n",
    "model.add(Dense(1, activation='sigmoid', input_dim=NUM_WORDS))\n",
    "```\n",
    "2. Задать функцию потерь и оптимизатор:\n",
    "```python\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "3. Запустить обучение:\n",
    "```python\n",
    "model.fit(X_train, y_train, \n",
    "          batch_size=32,\n",
    "          epochs=3,\n",
    "          validation_data=(X_test, y_test))\n",
    "```\n",
    "\n",
    "В NLP чаще всего ставятся задачи классификации, поэтому нужно запомнить такие функции потерь:\n",
    "\n",
    "*   **categorical_crossentropy** - для многоклассовой классификации, в качестве меток должны передаваться one-hot-encoding вектора\n",
    "*   **sparse_categorical_crossentropy** - аналогично предыдущему, но в качестве меток нужно передавать просто индексы соответствующих классов\n",
    "*   **binary_crossentropy** - для бинарной классификации\n",
    "\n",
    "\n",
    "В качестве оптимизатора обычно используют `sgd` или `adam`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1674992931542,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "Ncrq09zRxEvN",
    "outputId": "db306c67-f0df-4072-c666-55fe53b81373"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.14.0'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1674992931544,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "KddjCCo-w50h"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalMaxPooling1D, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 410,
     "status": "ok",
     "timestamp": 1674992931933,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "dDjri3167vFf",
    "outputId": "6abd1386-acc9-4231-c0bd-cdafd88ec6da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 1000, 64)          1308224   \n",
      "                                                                 \n",
      " global_max_pooling1d (Glob  (None, 64)                0         \n",
      " alMaxPooling1D)                                                 \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                650       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1308995 (4.99 MB)\n",
      "Trainable params: 1308995 (4.99 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Embedding(input_dim=len(word2idx), output_dim=64, input_shape=(X_train.shape[1],)),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(units=10, activation='relu'),\n",
    "    Dense(units=10, activation='relu'),\n",
    "    \n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41816,
     "status": "ok",
     "timestamp": 1674992973747,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "lw93gTmq8gZl",
    "outputId": "9581487a-0d88-4ed9-8766-f81155ea8ac9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "196/196 [==============================] - 44s 210ms/step - loss: 0.6096 - accuracy: 0.6934 - val_loss: 0.4227 - val_accuracy: 0.8506\n",
      "Epoch 2/10\n",
      "196/196 [==============================] - 48s 246ms/step - loss: 0.2913 - accuracy: 0.8890 - val_loss: 0.2857 - val_accuracy: 0.8805\n",
      "Epoch 3/10\n",
      "196/196 [==============================] - 47s 241ms/step - loss: 0.1766 - accuracy: 0.9343 - val_loss: 0.2827 - val_accuracy: 0.8846\n",
      "Epoch 4/10\n",
      "196/196 [==============================] - 48s 246ms/step - loss: 0.1067 - accuracy: 0.9668 - val_loss: 0.3031 - val_accuracy: 0.8826\n",
      "Epoch 5/10\n",
      "196/196 [==============================] - 50s 255ms/step - loss: 0.0589 - accuracy: 0.9850 - val_loss: 0.3357 - val_accuracy: 0.8778\n",
      "Epoch 6/10\n",
      "196/196 [==============================] - 40s 203ms/step - loss: 0.0288 - accuracy: 0.9948 - val_loss: 0.3757 - val_accuracy: 0.8738\n",
      "Epoch 7/10\n",
      "196/196 [==============================] - 37s 190ms/step - loss: 0.0132 - accuracy: 0.9989 - val_loss: 0.4086 - val_accuracy: 0.8727\n",
      "Epoch 8/10\n",
      "196/196 [==============================] - 42s 217ms/step - loss: 0.0061 - accuracy: 0.9998 - val_loss: 0.4381 - val_accuracy: 0.8728\n",
      "Epoch 9/10\n",
      "196/196 [==============================] - 37s 189ms/step - loss: 0.0031 - accuracy: 0.9999 - val_loss: 0.4624 - val_accuracy: 0.8732\n",
      "Epoch 10/10\n",
      "196/196 [==============================] - 40s 204ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.4841 - val_accuracy: 0.8720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1d212136990>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, \n",
    "          train_df.is_positive, \n",
    "          batch_size=128, \n",
    "          epochs=10, \n",
    "          validation_data=(X_test, test_df.is_positive))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yBGdVRQTyynD"
   },
   "source": [
    "**Задание** Подсчитайте качество модели на тесте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2791,
     "status": "ok",
     "timestamp": 1674992976515,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "iX7m5OPLBVRk",
    "outputId": "eaa843db-6504-4ee4-c5f7-a6ac97fa5670"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 11s 14ms/step - loss: 0.4841 - accuracy: 0.8720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4840625524520874, 0.8720399737358093]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, test_df.is_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6606,
     "status": "ok",
     "timestamp": 1674992983110,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "HTtgLEE6BqlF",
    "outputId": "934e182b-1bb0-4a3c-8dbb-8372d733d5ba"
   },
   "outputs": [],
   "source": [
    "X_train = convert(train_df.lemmatized, word2idx, 1000)\n",
    "X_test = convert(test_df.lemmatized, word2idx, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41083,
     "status": "ok",
     "timestamp": 1674993024174,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "DX4Jw5JABuTs",
    "outputId": "a28444a1-7cf2-46c4-da0f-116961ea8046"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "196/196 [==============================] - 41s 211ms/step - loss: 0.0278 - accuracy: 0.9911 - val_loss: 0.5255 - val_accuracy: 0.8624\n",
      "Epoch 2/10\n",
      "196/196 [==============================] - 44s 223ms/step - loss: 0.0051 - accuracy: 0.9994 - val_loss: 0.5602 - val_accuracy: 0.8622\n",
      "Epoch 3/10\n",
      "196/196 [==============================] - 41s 209ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.5789 - val_accuracy: 0.8627\n",
      "Epoch 4/10\n",
      "196/196 [==============================] - 44s 226ms/step - loss: 7.2463e-04 - accuracy: 1.0000 - val_loss: 0.5951 - val_accuracy: 0.8614\n",
      "Epoch 5/10\n",
      "196/196 [==============================] - 37s 191ms/step - loss: 5.2383e-04 - accuracy: 1.0000 - val_loss: 0.6090 - val_accuracy: 0.8616\n",
      "Epoch 6/10\n",
      "196/196 [==============================] - 38s 192ms/step - loss: 4.0240e-04 - accuracy: 1.0000 - val_loss: 0.6212 - val_accuracy: 0.8611\n",
      "Epoch 7/10\n",
      "196/196 [==============================] - 40s 204ms/step - loss: 3.1977e-04 - accuracy: 1.0000 - val_loss: 0.6338 - val_accuracy: 0.8612\n",
      "Epoch 8/10\n",
      "196/196 [==============================] - 39s 198ms/step - loss: 2.6054e-04 - accuracy: 1.0000 - val_loss: 0.6448 - val_accuracy: 0.8615\n",
      "Epoch 9/10\n",
      "196/196 [==============================] - 40s 205ms/step - loss: 2.1567e-04 - accuracy: 1.0000 - val_loss: 0.6556 - val_accuracy: 0.8608\n",
      "Epoch 10/10\n",
      "196/196 [==============================] - 45s 232ms/step - loss: 1.8045e-04 - accuracy: 1.0000 - val_loss: 0.6655 - val_accuracy: 0.8609\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1d2663a7450>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, \n",
    "          train_df.is_positive, \n",
    "          batch_size=128,\n",
    "          epochs=10, \n",
    "          validation_data=(X_test, test_df.is_positive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2668,
     "status": "ok",
     "timestamp": 1674993026825,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "ciPeuedJCEgF",
    "outputId": "71227e46-57e0-4d73-da7d-dd182ded247f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 9s 11ms/step - loss: 0.6655 - accuracy: 0.8609\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6655429005622864, 0.8609200119972229]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, test_df.is_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6462,
     "status": "ok",
     "timestamp": 1674993033270,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "ZLq-gijTCGjW",
    "outputId": "4decb51b-b980-45de-ba2e-99abb55921cf"
   },
   "outputs": [],
   "source": [
    "X_train = convert(train_df.stemmed, word2idx, 1000)\n",
    "X_test = convert(test_df.stemmed, word2idx, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20886,
     "status": "ok",
     "timestamp": 1674993054140,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "J9Qcdc6TCHwl",
    "outputId": "461aba02-414d-4305-edcc-ae0b3a18f46d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "196/196 [==============================] - 37s 191ms/step - loss: 0.2737 - accuracy: 0.9029 - val_loss: 0.4737 - val_accuracy: 0.8182\n",
      "Epoch 2/10\n",
      "196/196 [==============================] - 38s 192ms/step - loss: 0.1074 - accuracy: 0.9637 - val_loss: 0.5238 - val_accuracy: 0.8147\n",
      "Epoch 3/10\n",
      "196/196 [==============================] - 50s 253ms/step - loss: 0.0565 - accuracy: 0.9850 - val_loss: 0.5862 - val_accuracy: 0.8132\n",
      "Epoch 4/10\n",
      "196/196 [==============================] - 60s 307ms/step - loss: 0.0289 - accuracy: 0.9952 - val_loss: 0.6592 - val_accuracy: 0.8097\n",
      "Epoch 5/10\n",
      "196/196 [==============================] - 48s 245ms/step - loss: 0.0147 - accuracy: 0.9987 - val_loss: 0.7238 - val_accuracy: 0.8068\n",
      "Epoch 6/10\n",
      "196/196 [==============================] - 44s 222ms/step - loss: 0.0075 - accuracy: 0.9997 - val_loss: 0.7762 - val_accuracy: 0.8072\n",
      "Epoch 7/10\n",
      "196/196 [==============================] - 40s 205ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.8191 - val_accuracy: 0.8054\n",
      "Epoch 8/10\n",
      "196/196 [==============================] - 41s 210ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.8569 - val_accuracy: 0.8054\n",
      "Epoch 9/10\n",
      "196/196 [==============================] - 44s 222ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.8895 - val_accuracy: 0.8056\n",
      "Epoch 10/10\n",
      "196/196 [==============================] - 41s 208ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.9178 - val_accuracy: 0.8050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1d205e9f450>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, \n",
    "          train_df.is_positive, \n",
    "          batch_size=128, \n",
    "          epochs=10, \n",
    "          validation_data=(X_test, test_df.is_positive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2497,
     "status": "ok",
     "timestamp": 1674993056625,
     "user": {
      "displayName": "Maxim Gasilin",
      "userId": "15501735730920561422"
     },
     "user_tz": -180
    },
    "id": "-kTE8RuACLQN",
    "outputId": "3915553d-d471-49e8-86a7-bc0e1f0de953"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 8s 10ms/step - loss: 0.9178 - accuracy: 0.8050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9177547097206116, 0.8050400018692017]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, test_df.is_positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aCq2UPzlEqIm"
   },
   "source": [
    "Лучший score у сети на лемматизированных текстах"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
